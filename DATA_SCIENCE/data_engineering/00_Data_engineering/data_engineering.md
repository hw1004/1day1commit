# 데이터 엔지니어링
> **데이터 엔지니어링**: 실제 운영 환경에서 데이터 과학 및 분석에 필요한 기반을 구축하는 일련의 작업
> - 데이터 엔지니어링 유형
> 1. **SQL 중심**: 데이터의 작업 및 기본 저장소는 *관계형 데이터베이스*에 있으며 모든 데이터 처리는 SQL 기반 언어로 수행된다.
> 2. **빅데이터 중심**: 데이터 작업 및 기본 저장소는 *Hadoop, Cassandra, HBase*와 같은 기술에 기반한다. 모든 데이터 처리는 *MapReduce, Spark, Flink*와 같은 빅데이터 프레임워크에서 수행된다.
>
> - **전처리**: *수집 -> 저장 -> 처리*
>   - 수집, 저장, 처리에 필요한 작업의 흐름을 **Data PipeLine**이라고 한다.
>   - **수집**: 내/외부 데이터 연동 및 통합
>   - **저장**: 대용량/실시간 데이터 **분산** 저장
>   - **처리**: 데이터 선택, 변환, 통합, 축소
> - **후처리**: *처리 -> 탐색 -> 분석*
> - **표현**: *응용*
> - **전처리 => 후처리 => 표현**
> - 비정형 데이터는 구조화(결측치, 이상치 등 정제)가 필요함


## Flow (Building DataPipeLine)
1. 데이터 수집
   - 내부 데이터: 정형화된 데이터
   - 외부 데이터: 일련의 프로그램 통해서 주기적으로 수집 (**web crawling/web scraping**)
   - **Web crawling**: 웹상의 정보들을 탐색, 수집하는 작업
   - Web scraping: 특정 웹 사이트나 페이지에서 필요한 데이터를 자동 추출해 내는 것
2. 데이터 분산 저장 및 처리 기술
   - 저장용량 부하를 막기 위해 데이터를 여러개의 데이터 저장소에 분산 저장
   - **Hadoop**: 대용량 데이터를 분산 저장/처리할 수 있는 java 기반 open source Framework
   - **Spark**: 빅데이터 처리를 위한 오픈소스 고속 분산처리 엔진
   - RDD 기술
3. 스트리밍 데이터 및 중앙 일원화 파이프라인 처리 기술
   - **Kafka**: 고성능 분산 이벤트 스트리밍 플랫
4. Data 수집 및 저장 처리에 대한 관리 및 자동화 기술
   - **Airflow**: 데이터 파이프라인을 관리하기 위한 강력한 오픈소스 플렛폼


## Docker
> Linux container 기반 오픈소스 가상화 플랫폼 (Go 언어 기반)
>
> 실행 시점에 상관 없이 시점을 고정할 수 있다.
>
> - container => library, system equipment, code, runtime 포함되어 있음
>
> - Docker 사용하는 이유
> 1. 독립된 공간을 보장 받아 충돌이 발생하지 않는다.
> 2. container 내부에서 작업하고 배포하려면 도커 이미지로 만들어서 운영서버에 전달만 하면 된다.
> 3. 환경에 구애받지 않고 애플리케이션을 배포, 확장할 수 있고, 개발자들간의 환경설정을 동일하게 맞춰 사용할 수 있다.